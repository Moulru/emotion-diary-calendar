{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "obvflgH6r5V5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìå ÎîîÎ∞îÏù¥Ïä§: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# ‚úÖ Ï¥àÍ∏∞ ÏÑ§Ï†ï: Í∏∞Ï°¥ Î™®Îç∏ ÌååÏùº ÏÇ≠Ï†ú Î∞è ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï\n",
        "# for file in [\"photo_pretrained_dino.pth\", \"best_photo_model.pth\", \"best_drawing_model.pth\"]:\n",
        "    # if os.path.exists(file):\n",
        "        # os.remove(file)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üìå ÎîîÎ∞îÏù¥Ïä§: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EgSlz4Otr9yE"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞\n",
        "BATCH_SIZE = 16\n",
        "LR_PHOTO = 1e-4\n",
        "LR_DRAWING = 1e-5\n",
        "EPOCHS_PHOTO = 15\n",
        "EPOCHS_DRAWING = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TCaL60BGsBf4"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ CBAM Î™®Îìà Ï†ïÏùò\n",
        "# CNNÏùò feature mapÏóê ÎåÄÌï¥ Ï±ÑÎÑêÍ≥º Í≥µÍ∞Ñ Îã®ÏúÑÎ°ú Ï§ëÏöîÌïú Ï†ïÎ≥¥Î•º Í∞ïÏ°∞ÌïòÎäî attention Í∏∞Î≤ï\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)\n",
        "        )\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    # ÌèâÍ∑† poolingÍ≥º ÏµúÎåÄ pooling Í≤∞Í≥ºÎ•º MLPÏóê ÌÜµÍ≥ºÏãúÌÇ® Îí§ Îëê Í≤∞Í≥ºÎ•º ÎçîÌï¥ sigmoid Ï†ÅÏö©. Ïù¥ Í≤∞Í≥ºÎ•º attention maskÎ°ú ÏÇ¨Ïö©\n",
        "    # Í∞Å Ï±ÑÎÑêÏóê ÎåÄÌïú Ï§ëÏöîÎèÑÎ•º 0~1Î°ú ÌëúÌòÑÌïú tensor\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super().__init__()\n",
        "        padding = kernel_size // 2\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    # sigmoidÎ°ú [0,1] ÏÇ¨Ïù¥ Í∞íÏúºÎ°ú Ï†ïÍ∑úÌôîÎêú Í≥µÍ∞Ñ attention map ÏÉùÏÑ±\n",
        "    # Ïñ¥Îäê ÏúÑÏπòÍ∞Ä Ï§ëÏöîÌïúÏßÄ ÌëúÌòÑÎêú 2D mask\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "        return self.sigmoid(self.conv(x))\n",
        "\n",
        "# channelAttentionÏúºÎ°ú Ï±ÑÎÑêÏùÑ Í∞ïÏ°∞Ìïú Îí§, spatialAttentionÏúºÎ°ú ÏúÑÏπòÎ•º Í∞ïÏ°∞\n",
        "# Îëê Îã®Í≥Ñ Î™®Îëê ÏûÖÎ†• feature mapÍ≥º element-wise Í≥±ÏùÑ ÏàòÌñâ\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16, kernel_size=7):\n",
        "        super().__init__()\n",
        "        self.ca = ChannelAttention(in_channels, reduction)\n",
        "        self.sa = SpatialAttention(kernel_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x * self.ca(x)\n",
        "        x = x * self.sa(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnXZC1hDulv3"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TNpmx8YksESX"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ ResNet50 + CBAM Ï†ïÏùò\n",
        "# 4Í∞úÏùò ResNet block Î™®ÎëêÏóê CBAMÏùÑ ÏÇΩÏûÖÌï¥ Î∂ÑÎ•ò headÎ•º Í∞úÏÑ†\n",
        "class ResNet50_CBAM(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super().__init__()\n",
        "        # weights ÌååÎùºÎØ∏ÌÑ∞ ÏÇ¨Ïö© (pretrainedÎäî deprecated)\n",
        "        self.backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        # CBAMÏùÑ Ïó¨Îü¨ Ï∏µÏóê Ï∂îÍ∞Ä\n",
        "        # ResNet50ÏùÄ Í∞Å Î∏îÎ°ùÏùò Ï∂úÎ†• Ï±ÑÎÑê ÏàòÍ∞Ä ÏïÑÎûòÏôÄ Í∞ôÍ∏∞ ÎïåÎ¨∏Ïóê Ìï¥Îãπ in_channelsÎ°ú CBAM ÏÉùÏÑ±\n",
        "        # Í∞Å blockÏùò Ï∂úÎ†•Ïóê attentionÏùÑ Ï†ÅÏö©Ìï¥ Ï§ëÏöî Ï±ÑÎÑê Î∞è ÏúÑÏπò Í∞ïÏ°∞\n",
        "        self.cbam1 = CBAM(in_channels=256)   # layer1 output\n",
        "        self.cbam2 = CBAM(in_channels=512)   # layer2 output\n",
        "        self.cbam3 = CBAM(in_channels=1024)  # layer3 output\n",
        "        self.cbam4 = CBAM(in_channels=2048)  # layer4 output\n",
        "\n",
        "        # Í∏∞Ï°¥ FC layer Ï†úÍ±∞(Ïª§Ïä§ÌÖÄ classificationÏùÑ ÌôúÏö©Ìï† ÏòàÏ†ï)\n",
        "        self.backbone.fc = nn.Identity()\n",
        "\n",
        "        # classification head\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(2048, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ï¥àÍ∏∞ layers\n",
        "        x = self.backbone.conv1(x)\n",
        "        x = self.backbone.bn1(x)\n",
        "        x = self.backbone.relu(x)\n",
        "        x = self.backbone.maxpool(x)\n",
        "\n",
        "        # ResNet blocks with CBAM\n",
        "        x = self.backbone.layer1(x)\n",
        "        x = self.cbam1(x)\n",
        "\n",
        "        x = self.backbone.layer2(x)\n",
        "        x = self.cbam2(x)\n",
        "\n",
        "        x = self.backbone.layer3(x)\n",
        "        x = self.cbam3(x)\n",
        "\n",
        "        x = self.backbone.layer4(x)\n",
        "        x = self.cbam4(x)\n",
        "\n",
        "        # Global Average Pooling\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        return self.head(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_i73qv9nsIBE"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ Ï†ïÏùò\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.1),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1yQ26l2YsLpn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Í∞êÏ†ï ÌÅ¥ÎûòÏä§ (4Í∞ú): ['angry', 'fear', 'happy', 'sad']\n"
          ]
        }
      ],
      "source": [
        "photo_dataset_train = datasets.ImageFolder('datasets/dataset2yolo/photo/train', transform=transform_train)\n",
        "photo_dataset_val   = datasets.ImageFolder('datasets/dataset2yolo/photo/val', transform=transform_val)\n",
        "\n",
        "drawing_dataset_train = datasets.ImageFolder('datasets/dataset2yolo/drawing/train', transform=transform_train)\n",
        "drawing_dataset_val   = datasets.ImageFolder('datasets/dataset2yolo/drawing/val', transform=transform_val)\n",
        "\n",
        "class_names = photo_dataset_train.classes\n",
        "num_classes = len(class_names)\n",
        "print(f\"üìä Í∞êÏ†ï ÌÅ¥ÎûòÏä§ ({num_classes}Í∞ú): {class_names}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "e3LOcwBOsQA5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¨Í∏∞:\n",
            "ÏÇ¨ÏßÑ - ÌïôÏäµ: 5756, Í≤ÄÏ¶ù: 1440\n",
            "Í∑∏Î¶º - ÌïôÏäµ: 1539, Í≤ÄÏ¶ù: 472\n"
          ]
        }
      ],
      "source": [
        "# DataLoader ÏÉùÏÑ± (num_workers Ï°∞Ï†ï)\n",
        "num_workers = min(4, os.cpu_count() or 1)  # CPU ÏΩîÏñ¥ ÏàòÏóê ÎßûÍ≤å Ï°∞Ï†ï\n",
        "\n",
        "photo_train_loader = DataLoader(photo_dataset_train, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                               num_workers=num_workers, pin_memory=True if device.type == 'cuda' else False)\n",
        "photo_val_loader = DataLoader(photo_dataset_val, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                             num_workers=num_workers, pin_memory=True if device.type == 'cuda' else False)\n",
        "drawing_train_loader = DataLoader(drawing_dataset_train, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                                 num_workers=num_workers, pin_memory=True if device.type == 'cuda' else False)\n",
        "drawing_val_loader = DataLoader(drawing_dataset_val, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                               num_workers=num_workers, pin_memory=True if device.type == 'cuda' else False)\n",
        "\n",
        "print(f\"üìä Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¨Í∏∞:\")\n",
        "print(f\"ÏÇ¨ÏßÑ - ÌïôÏäµ: {len(photo_dataset_train)}, Í≤ÄÏ¶ù: {len(photo_dataset_val)}\")\n",
        "print(f\"Í∑∏Î¶º - ÌïôÏäµ: {len(drawing_dataset_train)}, Í≤ÄÏ¶ù: {len(drawing_dataset_val)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PMzfZWTOsT8Q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìå Î™®Îç∏Ïù¥ cudaÏóê Î°úÎìúÎêòÏóàÏäµÎãàÎã§.\n",
            "üìå Ï¥ù ÌååÎùºÎØ∏ÌÑ∞ Ïàò: 25,387,724\n",
            "üìå ÌïôÏäµ Í∞ÄÎä•Ìïú ÌååÎùºÎØ∏ÌÑ∞ Ïàò: 25,387,724\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ ÌïôÏäµ Î∞è Í≤ÄÏ¶ù Ìï®Ïàò Ï†ïÏùò\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f'Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            val_loss += criterion(output, target).item()\n",
        "\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(target.cpu().numpy())\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = 100. * correct / total\n",
        "\n",
        "    return val_loss, val_acc, all_preds, all_targets\n",
        "\n",
        "# ‚úÖ Î™®Îç∏ Ï¥àÍ∏∞Ìôî\n",
        "model = ResNet50_CBAM(num_classes=num_classes).to(device)\n",
        "print(f\"üìå Î™®Îç∏Ïù¥ {device}Ïóê Î°úÎìúÎêòÏóàÏäµÎãàÎã§.\")\n",
        "print(f\"üìå Ï¥ù ÌååÎùºÎØ∏ÌÑ∞ Ïàò: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"üìå ÌïôÏäµ Í∞ÄÎä•Ìïú ÌååÎùºÎØ∏ÌÑ∞ Ïàò: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üì∏ [1Îã®Í≥Ñ] ÏÇ¨ÏßÑ Îç∞Ïù¥ÌÑ∞Î°ú Pretraining ÏãúÏûë\n",
            "\n",
            "üîÅ Epoch 1/10\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 0/360, Loss: 1.1900\n",
            "Batch 10/360, Loss: 0.8686\n",
            "Batch 20/360, Loss: 1.1465\n",
            "Batch 30/360, Loss: 1.0513\n",
            "Batch 40/360, Loss: 0.8737\n",
            "Batch 50/360, Loss: 0.9113\n",
            "Batch 60/360, Loss: 1.2079\n",
            "Batch 70/360, Loss: 0.8352\n",
            "Batch 80/360, Loss: 0.8880\n",
            "Batch 90/360, Loss: 1.1265\n",
            "Batch 100/360, Loss: 0.6032\n",
            "Batch 110/360, Loss: 0.8664\n",
            "Batch 120/360, Loss: 0.6939\n",
            "Batch 130/360, Loss: 0.6617\n",
            "Batch 140/360, Loss: 1.1483\n",
            "Batch 150/360, Loss: 0.6316\n",
            "Batch 160/360, Loss: 0.7781\n",
            "Batch 170/360, Loss: 0.8857\n",
            "Batch 180/360, Loss: 0.4334\n",
            "Batch 190/360, Loss: 0.7495\n",
            "Batch 200/360, Loss: 1.1071\n",
            "Batch 210/360, Loss: 0.7944\n",
            "Batch 220/360, Loss: 0.8473\n",
            "Batch 230/360, Loss: 0.7920\n",
            "Batch 240/360, Loss: 0.8140\n",
            "Batch 250/360, Loss: 0.6153\n",
            "Batch 260/360, Loss: 0.7082\n",
            "Batch 270/360, Loss: 0.6944\n",
            "Batch 280/360, Loss: 1.1197\n",
            "Batch 290/360, Loss: 0.9973\n",
            "Batch 300/360, Loss: 0.6996\n",
            "Batch 310/360, Loss: 0.7607\n",
            "Batch 320/360, Loss: 0.6780\n",
            "Batch 330/360, Loss: 0.6776\n",
            "Batch 340/360, Loss: 0.7648\n",
            "Batch 350/360, Loss: 1.0489\n",
            "‚úÖ [Train] Loss: 0.8332 | Accuracy: 68.02%\n",
            "üéØ [Val]   Loss: 0.6721 | Accuracy: 73.33%\n",
            "\n",
            "üîÅ Epoch 2/10\n",
            "Batch 0/360, Loss: 0.6279\n",
            "Batch 10/360, Loss: 0.8342\n",
            "Batch 20/360, Loss: 0.7639\n",
            "Batch 30/360, Loss: 0.9952\n",
            "Batch 40/360, Loss: 0.5543\n",
            "Batch 50/360, Loss: 0.4603\n",
            "Batch 60/360, Loss: 0.5191\n",
            "Batch 70/360, Loss: 0.5642\n",
            "Batch 80/360, Loss: 0.6286\n",
            "Batch 90/360, Loss: 0.8637\n",
            "Batch 100/360, Loss: 1.1351\n",
            "Batch 110/360, Loss: 0.6719\n",
            "Batch 120/360, Loss: 0.6553\n",
            "Batch 130/360, Loss: 0.7137\n",
            "Batch 140/360, Loss: 0.7153\n",
            "Batch 150/360, Loss: 0.8384\n",
            "Batch 160/360, Loss: 0.7359\n",
            "Batch 170/360, Loss: 0.6230\n",
            "Batch 180/360, Loss: 0.8829\n",
            "Batch 190/360, Loss: 0.7773\n",
            "Batch 200/360, Loss: 0.9641\n",
            "Batch 210/360, Loss: 1.1694\n",
            "Batch 220/360, Loss: 0.5199\n",
            "Batch 230/360, Loss: 0.9062\n",
            "Batch 240/360, Loss: 0.8056\n",
            "Batch 250/360, Loss: 0.7563\n",
            "Batch 260/360, Loss: 0.9487\n",
            "Batch 270/360, Loss: 1.0120\n",
            "Batch 280/360, Loss: 0.7517\n",
            "Batch 290/360, Loss: 0.7494\n",
            "Batch 300/360, Loss: 0.5305\n",
            "Batch 310/360, Loss: 0.4917\n",
            "Batch 320/360, Loss: 0.7302\n",
            "Batch 330/360, Loss: 0.4071\n",
            "Batch 340/360, Loss: 0.9600\n",
            "Batch 350/360, Loss: 0.3919\n",
            "‚úÖ [Train] Loss: 0.6901 | Accuracy: 74.36%\n",
            "üéØ [Val]   Loss: 0.6794 | Accuracy: 73.33%\n",
            "\n",
            "üîÅ Epoch 3/10\n",
            "Batch 0/360, Loss: 0.8160\n",
            "Batch 10/360, Loss: 0.3789\n",
            "Batch 20/360, Loss: 0.5579\n",
            "Batch 30/360, Loss: 0.6752\n",
            "Batch 40/360, Loss: 0.7004\n",
            "Batch 50/360, Loss: 0.6211\n",
            "Batch 60/360, Loss: 0.5432\n",
            "Batch 70/360, Loss: 0.8717\n",
            "Batch 80/360, Loss: 0.6259\n",
            "Batch 90/360, Loss: 0.4123\n",
            "Batch 100/360, Loss: 1.0572\n",
            "Batch 110/360, Loss: 0.6294\n",
            "Batch 120/360, Loss: 0.3455\n",
            "Batch 130/360, Loss: 0.3191\n",
            "Batch 140/360, Loss: 0.9000\n",
            "Batch 150/360, Loss: 0.6222\n",
            "Batch 160/360, Loss: 0.5879\n",
            "Batch 170/360, Loss: 0.8191\n",
            "Batch 180/360, Loss: 0.3778\n",
            "Batch 190/360, Loss: 0.5957\n",
            "Batch 200/360, Loss: 0.5424\n",
            "Batch 210/360, Loss: 0.4375\n",
            "Batch 220/360, Loss: 0.4617\n",
            "Batch 230/360, Loss: 0.4058\n",
            "Batch 240/360, Loss: 0.6729\n",
            "Batch 250/360, Loss: 0.4220\n",
            "Batch 260/360, Loss: 0.3637\n",
            "Batch 270/360, Loss: 0.6358\n",
            "Batch 280/360, Loss: 0.7468\n",
            "Batch 290/360, Loss: 0.6139\n",
            "Batch 300/360, Loss: 0.4765\n",
            "Batch 310/360, Loss: 0.4755\n",
            "Batch 320/360, Loss: 0.5828\n",
            "Batch 330/360, Loss: 0.5947\n",
            "Batch 340/360, Loss: 0.3264\n",
            "Batch 350/360, Loss: 0.9333\n",
            "‚úÖ [Train] Loss: 0.6037 | Accuracy: 77.92%\n",
            "üéØ [Val]   Loss: 0.5900 | Accuracy: 77.78%\n",
            "\n",
            "üîÅ Epoch 4/10\n",
            "Batch 0/360, Loss: 0.6340\n",
            "Batch 10/360, Loss: 0.5235\n",
            "Batch 20/360, Loss: 0.7545\n",
            "Batch 30/360, Loss: 0.4333\n",
            "Batch 40/360, Loss: 0.4112\n",
            "Batch 50/360, Loss: 0.9431\n",
            "Batch 60/360, Loss: 0.6724\n",
            "Batch 70/360, Loss: 0.4487\n",
            "Batch 80/360, Loss: 0.8505\n",
            "Batch 90/360, Loss: 0.7427\n",
            "Batch 100/360, Loss: 0.6136\n",
            "Batch 110/360, Loss: 0.6452\n",
            "Batch 120/360, Loss: 0.5283\n",
            "Batch 130/360, Loss: 0.7858\n",
            "Batch 140/360, Loss: 0.7366\n",
            "Batch 150/360, Loss: 0.5626\n",
            "Batch 160/360, Loss: 0.5610\n",
            "Batch 170/360, Loss: 0.8669\n",
            "Batch 180/360, Loss: 0.3461\n",
            "Batch 190/360, Loss: 0.5505\n",
            "Batch 200/360, Loss: 0.8951\n",
            "Batch 210/360, Loss: 0.7313\n",
            "Batch 220/360, Loss: 0.3862\n",
            "Batch 230/360, Loss: 0.7622\n",
            "Batch 240/360, Loss: 0.4823\n",
            "Batch 250/360, Loss: 0.3235\n",
            "Batch 260/360, Loss: 0.7230\n",
            "Batch 270/360, Loss: 0.3963\n",
            "Batch 280/360, Loss: 0.6300\n",
            "Batch 290/360, Loss: 0.7064\n",
            "Batch 300/360, Loss: 0.6608\n",
            "Batch 310/360, Loss: 0.4654\n",
            "Batch 320/360, Loss: 0.4130\n",
            "Batch 330/360, Loss: 0.5336\n",
            "Batch 340/360, Loss: 0.6339\n",
            "Batch 350/360, Loss: 1.2187\n",
            "‚úÖ [Train] Loss: 0.5666 | Accuracy: 79.48%\n",
            "üéØ [Val]   Loss: 0.5289 | Accuracy: 79.86%\n",
            "\n",
            "üîÅ Epoch 5/10\n",
            "Batch 0/360, Loss: 0.5236\n",
            "Batch 10/360, Loss: 0.7958\n",
            "Batch 20/360, Loss: 0.3969\n",
            "Batch 30/360, Loss: 0.3400\n",
            "Batch 40/360, Loss: 0.7092\n",
            "Batch 50/360, Loss: 0.2618\n",
            "Batch 60/360, Loss: 0.2312\n",
            "Batch 70/360, Loss: 0.4575\n",
            "Batch 80/360, Loss: 0.6153\n",
            "Batch 90/360, Loss: 0.5412\n",
            "Batch 100/360, Loss: 0.3074\n",
            "Batch 110/360, Loss: 0.3267\n",
            "Batch 120/360, Loss: 0.4109\n",
            "Batch 130/360, Loss: 0.3968\n",
            "Batch 140/360, Loss: 0.4737\n",
            "Batch 150/360, Loss: 0.3187\n",
            "Batch 160/360, Loss: 1.0685\n",
            "Batch 170/360, Loss: 0.4474\n",
            "Batch 180/360, Loss: 0.7789\n",
            "Batch 190/360, Loss: 0.3993\n",
            "Batch 200/360, Loss: 0.4366\n",
            "Batch 210/360, Loss: 1.2002\n",
            "Batch 220/360, Loss: 0.3466\n",
            "Batch 230/360, Loss: 0.4129\n",
            "Batch 240/360, Loss: 0.5093\n",
            "Batch 250/360, Loss: 0.6948\n",
            "Batch 260/360, Loss: 1.0312\n",
            "Batch 270/360, Loss: 0.6380\n",
            "Batch 280/360, Loss: 0.6102\n",
            "Batch 290/360, Loss: 0.3642\n",
            "Batch 300/360, Loss: 0.4186\n",
            "Batch 310/360, Loss: 0.3505\n",
            "Batch 320/360, Loss: 0.3481\n",
            "Batch 330/360, Loss: 1.0623\n",
            "Batch 340/360, Loss: 1.2254\n",
            "Batch 350/360, Loss: 0.6309\n",
            "‚úÖ [Train] Loss: 0.5132 | Accuracy: 81.36%\n",
            "üéØ [Val]   Loss: 0.5798 | Accuracy: 78.47%\n",
            "\n",
            "üîÅ Epoch 6/10\n",
            "Batch 0/360, Loss: 0.4244\n",
            "Batch 10/360, Loss: 1.0566\n",
            "Batch 20/360, Loss: 1.4715\n",
            "Batch 30/360, Loss: 0.3828\n",
            "Batch 40/360, Loss: 0.8083\n",
            "Batch 50/360, Loss: 0.2842\n",
            "Batch 60/360, Loss: 0.4447\n",
            "Batch 70/360, Loss: 0.3534\n",
            "Batch 80/360, Loss: 0.2868\n",
            "Batch 90/360, Loss: 0.3237\n",
            "Batch 100/360, Loss: 0.3994\n",
            "Batch 110/360, Loss: 0.2866\n",
            "Batch 120/360, Loss: 0.6182\n",
            "Batch 130/360, Loss: 0.5515\n",
            "Batch 140/360, Loss: 0.4950\n",
            "Batch 150/360, Loss: 0.5670\n",
            "Batch 160/360, Loss: 0.4165\n",
            "Batch 170/360, Loss: 0.4723\n",
            "Batch 180/360, Loss: 0.3772\n",
            "Batch 190/360, Loss: 0.5471\n",
            "Batch 200/360, Loss: 0.9010\n",
            "Batch 210/360, Loss: 0.4414\n",
            "Batch 220/360, Loss: 0.2580\n",
            "Batch 230/360, Loss: 0.3922\n",
            "Batch 240/360, Loss: 0.5121\n",
            "Batch 250/360, Loss: 0.5233\n",
            "Batch 260/360, Loss: 0.3895\n",
            "Batch 270/360, Loss: 0.4690\n",
            "Batch 280/360, Loss: 0.9642\n",
            "Batch 290/360, Loss: 0.6214\n",
            "Batch 300/360, Loss: 0.2481\n",
            "Batch 310/360, Loss: 0.3266\n",
            "Batch 320/360, Loss: 0.3711\n",
            "Batch 330/360, Loss: 0.6997\n",
            "Batch 340/360, Loss: 0.2593\n",
            "Batch 350/360, Loss: 0.2312\n",
            "‚úÖ [Train] Loss: 0.4941 | Accuracy: 82.52%\n",
            "üéØ [Val]   Loss: 0.4991 | Accuracy: 82.43%\n",
            "\n",
            "üîÅ Epoch 7/10\n",
            "Batch 0/360, Loss: 0.4892\n",
            "Batch 10/360, Loss: 0.2846\n",
            "Batch 20/360, Loss: 0.3225\n",
            "Batch 30/360, Loss: 0.3755\n",
            "Batch 40/360, Loss: 0.2712\n",
            "Batch 50/360, Loss: 0.3598\n",
            "Batch 60/360, Loss: 0.2671\n",
            "Batch 70/360, Loss: 1.0209\n",
            "Batch 80/360, Loss: 0.5801\n",
            "Batch 90/360, Loss: 0.6662\n",
            "Batch 100/360, Loss: 0.5833\n",
            "Batch 110/360, Loss: 0.3512\n",
            "Batch 120/360, Loss: 0.1210\n",
            "Batch 130/360, Loss: 0.3561\n",
            "Batch 140/360, Loss: 0.5215\n",
            "Batch 150/360, Loss: 0.4330\n",
            "Batch 160/360, Loss: 0.1705\n",
            "Batch 170/360, Loss: 0.3592\n",
            "Batch 180/360, Loss: 0.6577\n",
            "Batch 190/360, Loss: 1.1234\n",
            "Batch 200/360, Loss: 0.4916\n",
            "Batch 210/360, Loss: 0.2371\n",
            "Batch 220/360, Loss: 0.4351\n",
            "Batch 230/360, Loss: 0.7383\n",
            "Batch 240/360, Loss: 0.4319\n",
            "Batch 250/360, Loss: 0.6848\n",
            "Batch 260/360, Loss: 0.6961\n",
            "Batch 270/360, Loss: 0.5535\n",
            "Batch 280/360, Loss: 0.3984\n",
            "Batch 290/360, Loss: 0.1737\n",
            "Batch 300/360, Loss: 0.4749\n",
            "Batch 310/360, Loss: 0.4898\n",
            "Batch 320/360, Loss: 0.3850\n",
            "Batch 330/360, Loss: 0.4261\n",
            "Batch 340/360, Loss: 0.5549\n",
            "Batch 350/360, Loss: 0.3085\n",
            "‚úÖ [Train] Loss: 0.4521 | Accuracy: 83.79%\n",
            "üéØ [Val]   Loss: 0.5197 | Accuracy: 80.49%\n",
            "\n",
            "üîÅ Epoch 8/10\n",
            "Batch 0/360, Loss: 0.4295\n",
            "Batch 10/360, Loss: 0.2650\n",
            "Batch 20/360, Loss: 0.8960\n",
            "Batch 30/360, Loss: 0.2653\n",
            "Batch 40/360, Loss: 0.2476\n",
            "Batch 50/360, Loss: 0.4919\n",
            "Batch 60/360, Loss: 0.5649\n",
            "Batch 70/360, Loss: 0.7491\n",
            "Batch 80/360, Loss: 0.6624\n",
            "Batch 90/360, Loss: 0.6041\n",
            "Batch 100/360, Loss: 0.3107\n",
            "Batch 110/360, Loss: 0.4641\n",
            "Batch 120/360, Loss: 0.2956\n",
            "Batch 130/360, Loss: 0.4742\n",
            "Batch 140/360, Loss: 0.2529\n",
            "Batch 150/360, Loss: 0.3831\n",
            "Batch 160/360, Loss: 0.3013\n",
            "Batch 170/360, Loss: 0.3404\n",
            "Batch 180/360, Loss: 0.2525\n",
            "Batch 190/360, Loss: 0.2158\n",
            "Batch 200/360, Loss: 0.3591\n",
            "Batch 210/360, Loss: 0.3413\n",
            "Batch 220/360, Loss: 1.1407\n",
            "Batch 230/360, Loss: 0.1815\n",
            "Batch 240/360, Loss: 0.1825\n",
            "Batch 250/360, Loss: 0.3114\n",
            "Batch 260/360, Loss: 0.4332\n",
            "Batch 270/360, Loss: 0.4602\n",
            "Batch 280/360, Loss: 0.1463\n",
            "Batch 290/360, Loss: 0.2967\n",
            "Batch 300/360, Loss: 0.3728\n",
            "Batch 310/360, Loss: 0.5563\n",
            "Batch 320/360, Loss: 0.1793\n",
            "Batch 330/360, Loss: 0.5767\n",
            "Batch 340/360, Loss: 0.2586\n",
            "Batch 350/360, Loss: 0.4261\n",
            "‚úÖ [Train] Loss: 0.4147 | Accuracy: 84.99%\n",
            "üéØ [Val]   Loss: 0.5085 | Accuracy: 80.49%\n",
            "\n",
            "üîÅ Epoch 9/10\n",
            "Batch 0/360, Loss: 0.2140\n",
            "Batch 10/360, Loss: 1.3653\n",
            "Batch 20/360, Loss: 0.6274\n",
            "Batch 30/360, Loss: 0.4794\n",
            "Batch 40/360, Loss: 0.2670\n",
            "Batch 50/360, Loss: 0.2799\n",
            "Batch 60/360, Loss: 0.2261\n",
            "Batch 70/360, Loss: 0.5202\n",
            "Batch 80/360, Loss: 0.7216\n",
            "Batch 90/360, Loss: 0.5272\n",
            "Batch 100/360, Loss: 0.4341\n",
            "Batch 110/360, Loss: 0.1859\n",
            "Batch 120/360, Loss: 0.6159\n",
            "Batch 130/360, Loss: 0.2553\n",
            "Batch 140/360, Loss: 0.4565\n",
            "Batch 150/360, Loss: 0.2371\n",
            "Batch 160/360, Loss: 0.3232\n",
            "Batch 170/360, Loss: 0.2249\n",
            "Batch 180/360, Loss: 0.1801\n",
            "Batch 190/360, Loss: 0.5180\n",
            "Batch 200/360, Loss: 0.2500\n",
            "Batch 210/360, Loss: 0.3126\n",
            "Batch 220/360, Loss: 0.4958\n",
            "Batch 230/360, Loss: 0.5575\n",
            "Batch 240/360, Loss: 0.4100\n",
            "Batch 250/360, Loss: 0.1602\n",
            "Batch 260/360, Loss: 0.3639\n",
            "Batch 270/360, Loss: 0.4537\n",
            "Batch 280/360, Loss: 0.2606\n",
            "Batch 290/360, Loss: 0.1299\n",
            "Batch 300/360, Loss: 0.3904\n",
            "Batch 310/360, Loss: 0.3419\n",
            "Batch 320/360, Loss: 0.3745\n",
            "Batch 330/360, Loss: 0.4452\n",
            "Batch 340/360, Loss: 0.5219\n",
            "Batch 350/360, Loss: 0.4874\n",
            "‚úÖ [Train] Loss: 0.3925 | Accuracy: 86.59%\n",
            "üéØ [Val]   Loss: 0.5350 | Accuracy: 80.76%\n",
            "\n",
            "üîÅ Epoch 10/10\n",
            "Batch 0/360, Loss: 0.2936\n",
            "Batch 10/360, Loss: 0.4004\n",
            "Batch 20/360, Loss: 0.3695\n",
            "Batch 30/360, Loss: 0.4399\n",
            "Batch 40/360, Loss: 0.3933\n",
            "Batch 50/360, Loss: 0.3193\n",
            "Batch 60/360, Loss: 0.1144\n",
            "Batch 70/360, Loss: 0.1683\n",
            "Batch 80/360, Loss: 0.1791\n",
            "Batch 90/360, Loss: 0.2354\n",
            "Batch 100/360, Loss: 0.2363\n",
            "Batch 110/360, Loss: 0.4080\n",
            "Batch 120/360, Loss: 0.1853\n",
            "Batch 130/360, Loss: 0.3473\n",
            "Batch 140/360, Loss: 0.9759\n",
            "Batch 150/360, Loss: 0.3266\n",
            "Batch 160/360, Loss: 0.2900\n",
            "Batch 170/360, Loss: 0.2581\n",
            "Batch 180/360, Loss: 0.1952\n",
            "Batch 190/360, Loss: 0.1661\n",
            "Batch 200/360, Loss: 0.2850\n",
            "Batch 210/360, Loss: 0.6191\n",
            "Batch 220/360, Loss: 0.2879\n",
            "Batch 230/360, Loss: 0.3192\n",
            "Batch 240/360, Loss: 0.6620\n",
            "Batch 250/360, Loss: 0.1593\n",
            "Batch 260/360, Loss: 0.2243\n",
            "Batch 270/360, Loss: 0.3601\n",
            "Batch 280/360, Loss: 0.4061\n",
            "Batch 290/360, Loss: 0.5035\n",
            "Batch 300/360, Loss: 0.3649\n",
            "Batch 310/360, Loss: 0.1299\n",
            "Batch 320/360, Loss: 0.2254\n",
            "Batch 330/360, Loss: 0.3613\n",
            "Batch 340/360, Loss: 0.5317\n",
            "Batch 350/360, Loss: 0.6378\n",
            "‚úÖ [Train] Loss: 0.3655 | Accuracy: 87.35%\n",
            "üéØ [Val]   Loss: 0.4951 | Accuracy: 81.67%\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "# ========================================\n",
        "# ‚úÖ 1Îã®Í≥Ñ: ÏÇ¨ÏßÑ Îç∞Ïù¥ÌÑ∞Î°ú Pretraining\n",
        "# ========================================\n",
        "print(\"\\nüì∏ [1Îã®Í≥Ñ] ÏÇ¨ÏßÑ Îç∞Ïù¥ÌÑ∞Î°ú Pretraining ÏãúÏûë\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "train_loader = photo_train_loader\n",
        "val_loader = photo_val_loader\n",
        "class_names = photo_dataset_train.classes\n",
        "\n",
        "EPOCHS_PRETRAIN = 10\n",
        "train_losses, train_accuracies = [], []\n",
        "val_losses, val_accuracies = [], []\n",
        "\n",
        "for epoch in range(EPOCHS_PRETRAIN):\n",
        "    print(f\"\\nüîÅ Epoch {epoch+1}/{EPOCHS_PRETRAIN}\")\n",
        "    \n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc, val_preds, val_targets = validate(model, val_loader, criterion, device)\n",
        "    \n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "    \n",
        "    print(f\"‚úÖ [Train] Loss: {train_loss:.4f} | Accuracy: {train_acc:.2f}%\")\n",
        "    print(f\"üéØ [Val]   Loss: {val_loss:.4f} | Accuracy: {val_acc:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üñºÔ∏è [2Îã®Í≥Ñ] Í∑∏Î¶º Îç∞Ïù¥ÌÑ∞Î°ú Fine-tuning ÏãúÏûë\n",
            "\n",
            "üîÅ Epoch 1/10\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 0/97, Loss: 2.8488\n",
            "Batch 10/97, Loss: 2.7068\n",
            "Batch 20/97, Loss: 3.2304\n",
            "Batch 30/97, Loss: 1.1919\n",
            "Batch 40/97, Loss: 3.0619\n",
            "Batch 50/97, Loss: 2.6732\n",
            "Batch 60/97, Loss: 4.1255\n",
            "Batch 70/97, Loss: 2.8306\n",
            "Batch 80/97, Loss: 2.0819\n",
            "Batch 90/97, Loss: 3.1861\n",
            "‚úÖ [FT-Train] Loss: 2.9074 | Accuracy: 30.60%\n",
            "üéØ [FT-Val]   Loss: 3.1832 | Accuracy: 25.00%\n",
            "üíæ Best model saved! (val_acc = 25.00%)\n",
            "\n",
            "üîÅ Epoch 2/10\n",
            "Batch 0/97, Loss: 2.7312\n",
            "Batch 10/97, Loss: 2.6045\n",
            "Batch 20/97, Loss: 3.4177\n",
            "Batch 30/97, Loss: 3.0810\n",
            "Batch 40/97, Loss: 2.9214\n",
            "Batch 50/97, Loss: 2.4789\n",
            "Batch 60/97, Loss: 3.3856\n",
            "Batch 70/97, Loss: 3.6771\n",
            "Batch 80/97, Loss: 3.8162\n",
            "Batch 90/97, Loss: 3.2634\n",
            "‚úÖ [FT-Train] Loss: 2.8872 | Accuracy: 30.21%\n",
            "üéØ [FT-Val]   Loss: 3.1950 | Accuracy: 23.52%\n",
            "‚è≥ No improvement. Patience: 1/3\n",
            "\n",
            "üîÅ Epoch 3/10\n",
            "Batch 0/97, Loss: 3.5186\n",
            "Batch 10/97, Loss: 2.3872\n",
            "Batch 20/97, Loss: 4.2308\n",
            "Batch 30/97, Loss: 3.2246\n",
            "Batch 40/97, Loss: 2.1315\n",
            "Batch 50/97, Loss: 3.9645\n",
            "Batch 60/97, Loss: 2.6712\n",
            "Batch 70/97, Loss: 3.3506\n",
            "Batch 80/97, Loss: 2.2725\n",
            "Batch 90/97, Loss: 3.0896\n",
            "‚úÖ [FT-Train] Loss: 2.8475 | Accuracy: 28.72%\n",
            "üéØ [FT-Val]   Loss: 3.0683 | Accuracy: 24.15%\n",
            "‚è≥ No improvement. Patience: 2/3\n",
            "\n",
            "üîÅ Epoch 4/10\n",
            "Batch 0/97, Loss: 3.2055\n",
            "Batch 10/97, Loss: 2.0604\n",
            "Batch 20/97, Loss: 3.4154\n",
            "Batch 30/97, Loss: 2.8746\n",
            "Batch 40/97, Loss: 2.3064\n",
            "Batch 50/97, Loss: 2.6031\n",
            "Batch 60/97, Loss: 2.4352\n",
            "Batch 70/97, Loss: 3.4158\n",
            "Batch 80/97, Loss: 3.4020\n",
            "Batch 90/97, Loss: 1.9219\n",
            "‚úÖ [FT-Train] Loss: 2.8327 | Accuracy: 31.12%\n",
            "üéØ [FT-Val]   Loss: 2.9604 | Accuracy: 25.64%\n",
            "üíæ Best model saved! (val_acc = 25.64%)\n",
            "\n",
            "üîÅ Epoch 5/10\n",
            "Batch 0/97, Loss: 2.8705\n",
            "Batch 10/97, Loss: 3.1877\n",
            "Batch 20/97, Loss: 2.5486\n",
            "Batch 30/97, Loss: 2.6710\n",
            "Batch 40/97, Loss: 2.7932\n",
            "Batch 50/97, Loss: 1.6563\n",
            "Batch 60/97, Loss: 4.0796\n",
            "Batch 70/97, Loss: 2.5169\n",
            "Batch 80/97, Loss: 4.0432\n",
            "Batch 90/97, Loss: 3.1892\n",
            "‚úÖ [FT-Train] Loss: 2.7823 | Accuracy: 31.12%\n",
            "üéØ [FT-Val]   Loss: 3.1346 | Accuracy: 25.85%\n",
            "üíæ Best model saved! (val_acc = 25.85%)\n",
            "\n",
            "üîÅ Epoch 6/10\n",
            "Batch 0/97, Loss: 2.4209\n",
            "Batch 10/97, Loss: 3.3052\n",
            "Batch 20/97, Loss: 2.1263\n",
            "Batch 30/97, Loss: 2.8473\n",
            "Batch 40/97, Loss: 3.2103\n",
            "Batch 50/97, Loss: 2.9257\n",
            "Batch 60/97, Loss: 3.7098\n",
            "Batch 70/97, Loss: 3.6184\n",
            "Batch 80/97, Loss: 3.2188\n",
            "Batch 90/97, Loss: 2.2619\n",
            "‚úÖ [FT-Train] Loss: 2.8050 | Accuracy: 30.80%\n",
            "üéØ [FT-Val]   Loss: 3.1242 | Accuracy: 25.00%\n",
            "‚è≥ No improvement. Patience: 1/3\n",
            "\n",
            "üîÅ Epoch 7/10\n",
            "Batch 0/97, Loss: 1.9820\n",
            "Batch 10/97, Loss: 2.0224\n",
            "Batch 20/97, Loss: 2.1913\n",
            "Batch 30/97, Loss: 2.7105\n",
            "Batch 40/97, Loss: 2.3644\n",
            "Batch 50/97, Loss: 2.5235\n",
            "Batch 60/97, Loss: 2.2151\n",
            "Batch 70/97, Loss: 2.6961\n",
            "Batch 80/97, Loss: 2.3552\n",
            "Batch 90/97, Loss: 3.9407\n",
            "‚úÖ [FT-Train] Loss: 2.7671 | Accuracy: 32.16%\n",
            "üéØ [FT-Val]   Loss: 3.0856 | Accuracy: 26.48%\n",
            "üíæ Best model saved! (val_acc = 26.48%)\n",
            "\n",
            "üîÅ Epoch 8/10\n",
            "Batch 0/97, Loss: 2.5370\n",
            "Batch 10/97, Loss: 2.8038\n",
            "Batch 20/97, Loss: 3.2254\n",
            "Batch 30/97, Loss: 2.9920\n",
            "Batch 40/97, Loss: 3.3901\n",
            "Batch 50/97, Loss: 2.2122\n",
            "Batch 60/97, Loss: 2.3140\n",
            "Batch 70/97, Loss: 2.4491\n",
            "Batch 80/97, Loss: 1.9244\n",
            "Batch 90/97, Loss: 2.3118\n",
            "‚úÖ [FT-Train] Loss: 2.7894 | Accuracy: 30.73%\n",
            "üéØ [FT-Val]   Loss: 2.9713 | Accuracy: 27.54%\n",
            "üíæ Best model saved! (val_acc = 27.54%)\n",
            "\n",
            "üîÅ Epoch 9/10\n",
            "Batch 0/97, Loss: 3.9530\n",
            "Batch 10/97, Loss: 2.5497\n",
            "Batch 20/97, Loss: 2.4566\n",
            "Batch 30/97, Loss: 1.6716\n",
            "Batch 40/97, Loss: 2.9627\n",
            "Batch 50/97, Loss: 3.8181\n",
            "Batch 60/97, Loss: 2.7749\n",
            "Batch 70/97, Loss: 3.0036\n",
            "Batch 80/97, Loss: 2.6464\n",
            "Batch 90/97, Loss: 2.3043\n",
            "‚úÖ [FT-Train] Loss: 2.7089 | Accuracy: 33.14%\n",
            "üéØ [FT-Val]   Loss: 3.2394 | Accuracy: 26.48%\n",
            "‚è≥ No improvement. Patience: 1/3\n",
            "\n",
            "üîÅ Epoch 10/10\n",
            "Batch 0/97, Loss: 2.9463\n",
            "Batch 10/97, Loss: 2.5323\n",
            "Batch 20/97, Loss: 2.4749\n",
            "Batch 30/97, Loss: 2.7748\n",
            "Batch 40/97, Loss: 4.3753\n",
            "Batch 50/97, Loss: 2.2394\n",
            "Batch 60/97, Loss: 3.0570\n",
            "Batch 70/97, Loss: 3.3565\n",
            "Batch 80/97, Loss: 1.9422\n",
            "Batch 90/97, Loss: 2.5514\n",
            "‚úÖ [FT-Train] Loss: 2.6943 | Accuracy: 33.72%\n",
            "üéØ [FT-Val]   Loss: 3.3176 | Accuracy: 28.18%\n",
            "üíæ Best model saved! (val_acc = 28.18%)\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# ‚úÖ 2Îã®Í≥Ñ: Í∑∏Î¶º Îç∞Ïù¥ÌÑ∞Î°ú Fine-tuning\n",
        "# ========================================\n",
        "print(\"\\nüñºÔ∏è [2Îã®Í≥Ñ] Í∑∏Î¶º Îç∞Ïù¥ÌÑ∞Î°ú Fine-tuning ÏãúÏûë\")\n",
        "\n",
        "# (ÏÑ†ÌÉù) ÌäπÏ†ï Î†àÏù¥Ïñ¥ Freeze\n",
        "for name, param in model.named_parameters():\n",
        "    if 'fc' not in name:  # Î∂ÑÎ•ò Ìó§Îìú Ï†úÏô∏ÌïòÍ≥† Freeze\n",
        "        param.requires_grad = False\n",
        "\n",
        "# (ÌïÑÏàò) ÏòµÌã∞ÎßàÏù¥Ï†Ä Îã§Ïãú Ï†ïÏùò (freeze Î∞òÏòÅ)\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
        "\n",
        "# Í∑∏Î¶º Îç∞Ïù¥ÌÑ∞ DataLoader Ï†ÅÏö©\n",
        "train_loader = drawing_train_loader\n",
        "val_loader = drawing_val_loader\n",
        "class_names = drawing_dataset_train.classes\n",
        "\n",
        "EPOCHS_FINETUNE = 10\n",
        "ft_train_losses, ft_train_accuracies = [], []\n",
        "ft_val_losses, ft_val_accuracies = [], []\n",
        "\n",
        "best_acc = 0.0\n",
        "patience = 3\n",
        "wait = 0\n",
        "\n",
        "for epoch in range(EPOCHS_FINETUNE):\n",
        "    print(f\"\\nüîÅ Epoch {epoch+1}/{EPOCHS_FINETUNE}\")\n",
        "\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc, val_preds, val_targets = validate(model, val_loader, criterion, device)\n",
        "\n",
        "    ft_train_losses.append(train_loss)\n",
        "    ft_train_accuracies.append(train_acc)\n",
        "    ft_val_losses.append(val_loss)\n",
        "    ft_val_accuracies.append(val_acc)\n",
        "\n",
        "    print(f\"‚úÖ [FT-Train] Loss: {train_loss:.4f} | Accuracy: {train_acc:.2f}%\")\n",
        "    print(f\"üéØ [FT-Val]   Loss: {val_loss:.4f} | Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        wait = 0\n",
        "        torch.save(model.state_dict(), \"/workspace/models/best_CNNattention_drawing_model.pth\")\n",
        "        print(f\"üíæ Best model saved! (val_acc = {val_acc:.2f}%)\")\n",
        "    else:\n",
        "        wait += 1\n",
        "        print(f\"‚è≥ No improvement. Patience: {wait}/{patience}\")\n",
        "        if wait >= patience:\n",
        "            print(\"üõë Early stopping triggered.\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìã Classification Report (Per Class):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       angry     0.2566    0.3900    0.3095       100\n",
            "        fear     0.1624    0.2043    0.1810        93\n",
            "       happy     0.4767    0.2253    0.3060       182\n",
            "         sad     0.2906    0.3505    0.3178        97\n",
            "\n",
            "    accuracy                         0.2818       472\n",
            "   macro avg     0.2966    0.2925    0.2786       472\n",
            "weighted avg     0.3299    0.2818    0.2845       472\n",
            "\n",
            "\n",
            "‚úÖ Overall Accuracy: 28.18%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Îî∞Î°ú Ïã§ÌñâÌïòÍ≥† Ïã∂ÏùÑ Îïå ÏÇ¨Ïö©. tuning Ïù¥ÌõÑ Î∞îÎ°ú Ïã§Ìñâ Ïãú ÏïÑÎûò ÏΩîÎìú Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©.\n",
        "# model.load_state_dict(torch.load(\"/workspace/models/best_CNNattention_drawing_model.pth\"))\n",
        "# model.eval()\n",
        "# val_loss, val_acc, val_preds, val_targets = validate(model, val_loader, criterion, device)\n",
        "\n",
        "print(\"\\nüìã Classification Report (Per Class):\")\n",
        "report = classification_report(\n",
        "    val_targets,\n",
        "    val_preds,\n",
        "    target_names=class_names,\n",
        "    digits=4\n",
        ")\n",
        "print(report)\n",
        "\n",
        "accuracy = accuracy_score(val_targets, val_preds)\n",
        "print(f\"\\n‚úÖ Overall Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
