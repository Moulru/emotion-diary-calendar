{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k1bbpB4pqC4w"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/workspace/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Œ ë””ë°”ì´ìŠ¤: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "from sklearn.metrics import classification_report\n",
        "import timm\n",
        "import numpy as np\n",
        "\n",
        "# ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì‚­ì œ\n",
        "for file in [\"photo_pretrained_dino.pth\", \"best_photo_model.pth\", \"best_drawing_model.pth\"]:\n",
        "    if os.path.exists(file):\n",
        "        os.remove(file)\n",
        "\n",
        "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ğŸ“Œ ë””ë°”ì´ìŠ¤: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aV3F48qhqK7q"
      },
      "outputs": [],
      "source": [
        "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "BATCH_SIZE = 16\n",
        "LR_PHOTO = 1e-4\n",
        "LR_DRAWING = 1e-5\n",
        "EPOCHS_PHOTO = 15\n",
        "EPOCHS_DRAWING = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "J5mG6s3RqME0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°:\n",
            "ì‚¬ì§„ - í•™ìŠµ: 5756, ê²€ì¦: 1440\n",
            "ê·¸ë¦¼ - í•™ìŠµ: 1608, ê²€ì¦: 403\n",
            "í´ë˜ìŠ¤: ['train', 'val']\n"
          ]
        }
      ],
      "source": [
        "# 224x224 ì „ì²˜ë¦¬ (DINOv2 ìµœì í™”)\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# train/val ë‚˜ëˆ„ê¸° (ì¼ê´€ëœ ë¶„í• )\n",
        "def split_dataset_consistent(dataset_train, dataset_val):\n",
        "    total_size = len(dataset_train)\n",
        "    train_size = int(0.8 * total_size)\n",
        "    val_size = total_size - train_size\n",
        "\n",
        "    generator = torch.Generator().manual_seed(42)\n",
        "    train_indices, val_indices = random_split(range(total_size), [train_size, val_size], generator=generator)\n",
        "\n",
        "    train_subset = torch.utils.data.Subset(dataset_train, train_indices.indices)\n",
        "    val_subset = torch.utils.data.Subset(dataset_val, val_indices.indices)\n",
        "\n",
        "    return train_subset, val_subset\n",
        "\n",
        "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
        "photo_dataset_train = datasets.ImageFolder('datasets/dataset2yolo/photo', transform=transform_train)\n",
        "photo_dataset_val = datasets.ImageFolder('datasets/dataset2yolo/photo', transform=transform_val)\n",
        "drawing_dataset_train = datasets.ImageFolder('datasets/dataset2yolo/drawing', transform=transform_train)\n",
        "drawing_dataset_val = datasets.ImageFolder('datasets/dataset2yolo/drawing', transform=transform_val)\n",
        "\n",
        "photo_train, photo_val = split_dataset_consistent(photo_dataset_train, photo_dataset_val)\n",
        "drawing_train, drawing_val = split_dataset_consistent(drawing_dataset_train, drawing_dataset_val)\n",
        "\n",
        "# ë°ì´í„° ë¡œë”\n",
        "photo_train_loader = DataLoader(photo_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "photo_val_loader = DataLoader(photo_val, batch_size=BATCH_SIZE, num_workers=4)\n",
        "drawing_train_loader = DataLoader(drawing_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "drawing_val_loader = DataLoader(drawing_val, batch_size=BATCH_SIZE, num_workers=4)\n",
        "\n",
        "print(f\"ğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°:\")\n",
        "print(f\"ì‚¬ì§„ - í•™ìŠµ: {len(photo_train)}, ê²€ì¦: {len(photo_val)}\")\n",
        "print(f\"ê·¸ë¦¼ - í•™ìŠµ: {len(drawing_train)}, ê²€ì¦: {len(drawing_val)}\")\n",
        "print(f\"í´ë˜ìŠ¤: {photo_dataset_train.classes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC97lwhgqYDy"
      },
      "source": [
        "### Dinov2 + Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9SbvwDwhqoK"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cKMlypXSqRb8"
      },
      "outputs": [],
      "source": [
        "class DinoTransformerClassifier(nn.Module):\n",
        "    def __init__(self, model_name='vit_small_patch14_dinov2.lvd142m', num_classes=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        # DINOv2 ë°±ë³¸ ë¡œë“œ (ì‚¬ì „í›ˆë ¨ëœ íŠ¹ì„± ì¶”ì¶œê¸°)\n",
        "        self.backbone = timm.create_model(model_name, pretrained=True, img_size=224)\n",
        "        self.backbone.reset_classifier(0)  # ê¸°ì¡´ ë¶„ë¥˜ê¸° ì œê±°\n",
        "        \n",
        "        # ì´ˆê¸°ì—ëŠ” ë°±ë³¸ ê°€ì¤‘ì¹˜ ê³ ì • (íŠ¹ì„± ì¶”ì¶œê¸°ë¡œë§Œ ì‚¬ìš©)\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # ë¶„ë¥˜ í—¤ë“œ (ê°œì„ ëœ êµ¬ì¡°)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(self.backbone.num_features),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(self.backbone.num_features, 512),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout * 1.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # DINOv2 íŠ¹ì„± ì¶”ì¶œ\n",
        "        features = self.backbone.forward_features(x)  # [B, num_patches+1, embed_dim]\n",
        "        \n",
        "        # CLS í† í° ì¶”ì¶œ (ì²« ë²ˆì§¸ í† í°)\n",
        "        cls_token = features[:, 0]  # [B, embed_dim]\n",
        "        \n",
        "        # ë¶„ë¥˜ í—¤ë“œ í†µê³¼\n",
        "        return self.head(cls_token)\n",
        "\n",
        "    def unfreeze_backbone_layers(self, num_layers=2):\n",
        "        \"\"\"ë°±ë³¸ì˜ ë§ˆì§€ë§‰ ë ˆì´ì–´ë“¤ì„ ì–¸í”„ë¦¬ì§•\"\"\"\n",
        "        total_blocks = len(self.backbone.blocks)\n",
        "        for i in range(total_blocks - num_layers, total_blocks):\n",
        "            for param in self.backbone.blocks[i].parameters():\n",
        "                param.requires_grad = True\n",
        "        \n",
        "        # norm layerë„ ì–¸í”„ë¦¬ì§•\n",
        "        for param in self.backbone.norm.parameters():\n",
        "            param.requires_grad = True\n",
        "        \n",
        "        print(f\"ğŸ”“ ë°±ë³¸ì˜ ë§ˆì§€ë§‰ {num_layers}ê°œ ë ˆì´ì–´ ì–¸í”„ë¦¬ì§• ì™„ë£Œ\")\n",
        "\n",
        "# ì†ì‹¤í•¨ìˆ˜\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "def train_epoch(model, loader, optimizer, criterion, scheduler=None):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        \n",
        "        # Gradient clipping (ì•ˆì •ì ì¸ í•™ìŠµ)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    if scheduler:\n",
        "        scheduler.step()\n",
        "\n",
        "    return running_loss / len(loader), 100. * correct / total\n",
        "\n",
        "def evaluate_model(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            \n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return running_loss / len(loader), 100. * correct / total, all_predictions, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aiNBLEUfqr7Q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸš€ Step 1: ì‚¬ì§„ ë°ì´í„°ë¡œ ì´ˆê¸° í•™ìŠµ ì‹œì‘\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[ğŸ“¸ Step1 ì—í­ 1/15]\n",
            "í›ˆë ¨ ì†ì‹¤: 0.5473, í›ˆë ¨ ì •í™•ë„: 79.93%\n",
            "ê²€ì¦ ì†ì‹¤: 0.5447, ê²€ì¦ ì •í™•ë„: 79.79%\n",
            "âœ… best_photo_model.pth ì €ì¥ë¨\n",
            "\n",
            "[ğŸ“¸ Step1 ì—í­ 2/15]\n",
            "í›ˆë ¨ ì†ì‹¤: 0.5389, í›ˆë ¨ ì •í™•ë„: 80.04%\n",
            "ê²€ì¦ ì†ì‹¤: 0.5425, ê²€ì¦ ì •í™•ë„: 79.79%\n",
            "\n",
            "[ğŸ“¸ Step1 ì—í­ 3/15]\n",
            "í›ˆë ¨ ì†ì‹¤: 0.5355, í›ˆë ¨ ì •í™•ë„: 80.04%\n",
            "ê²€ì¦ ì†ì‹¤: 0.5432, ê²€ì¦ ì •í™•ë„: 79.79%\n",
            "\n",
            "[ğŸ“¸ Step1 ì—í­ 4/15]\n",
            "í›ˆë ¨ ì†ì‹¤: 0.5332, í›ˆë ¨ ì •í™•ë„: 80.04%\n",
            "ê²€ì¦ ì†ì‹¤: 0.5477, ê²€ì¦ ì •í™•ë„: 79.79%\n",
            "\n",
            "[ğŸ“¸ Step1 ì—í­ 5/15]\n",
            "í›ˆë ¨ ì†ì‹¤: 0.5305, í›ˆë ¨ ì •í™•ë„: 80.04%\n",
            "ê²€ì¦ ì†ì‹¤: 0.5519, ê²€ì¦ ì •í™•ë„: 79.79%\n",
            "\n",
            "[ğŸ“¸ Step1 ì—í­ 6/15]\n",
            "í›ˆë ¨ ì†ì‹¤: 0.5271, í›ˆë ¨ ì •í™•ë„: 80.06%\n",
            "ê²€ì¦ ì†ì‹¤: 0.5496, ê²€ì¦ ì •í™•ë„: 79.72%\n",
            "\n",
            "[ğŸ“¸ Step1 ì—í­ 7/15]\n",
            "í›ˆë ¨ ì†ì‹¤: 0.5230, í›ˆë ¨ ì •í™•ë„: 80.04%\n",
            "ê²€ì¦ ì†ì‹¤: 0.5497, ê²€ì¦ ì •í™•ë„: 79.72%\n",
            "\n",
            "[ğŸ“¸ Step1 ì—í­ 8/15]\n",
            "í›ˆë ¨ ì†ì‹¤: 0.5183, í›ˆë ¨ ì •í™•ë„: 80.09%\n",
            "ê²€ì¦ ì†ì‹¤: 0.5541, ê²€ì¦ ì •í™•ë„: 79.72%\n",
            "â° ì¡°ê¸° ì¢…ë£Œ\n"
          ]
        }
      ],
      "source": [
        "# Step 1: ì‚¬ì§„ ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ\n",
        "print(\"\\nğŸš€ Step 1: ì‚¬ì§„ ë°ì´í„°ë¡œ ì´ˆê¸° í•™ìŠµ ì‹œì‘\")\n",
        "model = DinoTransformerClassifier(num_classes=len(photo_dataset_train.classes)).to(device)\n",
        "\n",
        "# í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë§Œ ìµœì í™” (ì²˜ìŒì—ëŠ” headë§Œ)\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.AdamW(trainable_params, lr=LR_PHOTO, weight_decay=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_PHOTO)\n",
        "\n",
        "best_val_acc, patience_counter = 0, 0\n",
        "\n",
        "for epoch in range(EPOCHS_PHOTO):\n",
        "    print(f\"\\n[ğŸ“¸ Step1 ì—í­ {epoch+1}/{EPOCHS_PHOTO}]\")\n",
        "    train_loss, train_acc = train_epoch(model, photo_train_loader, optimizer, criterion, scheduler)\n",
        "    val_loss, val_acc, _, _ = evaluate_model(model, photo_val_loader, criterion)\n",
        "\n",
        "    print(f\"í›ˆë ¨ ì†ì‹¤: {train_loss:.4f}, í›ˆë ¨ ì •í™•ë„: {train_acc:.2f}%\")\n",
        "    print(f\"ê²€ì¦ ì†ì‹¤: {val_loss:.4f}, ê²€ì¦ ì •í™•ë„: {val_acc:.2f}%\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_photo_model.pth\")\n",
        "        patience_counter = 0\n",
        "        print(\"âœ… best_photo_model.pth ì €ì¥ë¨\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= 7:\n",
        "            print(\"â° ì¡°ê¸° ì¢…ë£Œ\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PV6_VvliquoZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ¨ Step 2: ê·¸ë¦¼ ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹ ì‹œì‘ (ìµœê³  ì‚¬ì§„ ì •í™•ë„: 79.79%)\n",
            "\n",
            "[ğŸ¨ Step2 ì—í­ 1/30]\n",
            "í›ˆë ¨ ì†ì‹¤: 0.5845, í›ˆë ¨ ì •í™•ë„: 76.62%\n",
            "ê²€ì¦ ì†ì‹¤: 0.5870, ê²€ì¦ ì •í™•ë„: 76.18%\n",
            "âœ… best_drawing_model.pth ì €ì¥ë¨\n",
            "\n",
            "[ğŸ¨ Step2 ì—í­ 2/30]\n",
            "í›ˆë ¨ ì†ì‹¤: 0.5766, í›ˆë ¨ ì •í™•ë„: 76.62%\n",
            "ê²€ì¦ ì†ì‹¤: 0.5831, ê²€ì¦ ì •í™•ë„: 76.18%\n",
            "\n",
            "[ğŸ¨ Step2 ì—í­ 3/30]\n",
            "í›ˆë ¨ ì†ì‹¤: 0.5759, í›ˆë ¨ ì •í™•ë„: 76.62%\n",
            "ê²€ì¦ ì†ì‹¤: 0.5822, ê²€ì¦ ì •í™•ë„: 76.18%\n",
            "\n",
            "[ğŸ¨ Step2 ì—í­ 4/30]\n",
            "í›ˆë ¨ ì†ì‹¤: 0.5753, í›ˆë ¨ ì •í™•ë„: 76.62%\n",
            "ê²€ì¦ ì†ì‹¤: 0.5818, ê²€ì¦ ì •í™•ë„: 76.18%\n",
            "\n",
            "[ğŸ¨ Step2 ì—í­ 5/30]\n",
            "í›ˆë ¨ ì†ì‹¤: 0.5744, í›ˆë ¨ ì •í™•ë„: 76.62%\n",
            "ê²€ì¦ ì†ì‹¤: 0.5819, ê²€ì¦ ì •í™•ë„: 76.18%\n",
            "\n",
            "[ğŸ¨ Step2 ì—í­ 6/30]\n",
            "í›ˆë ¨ ì†ì‹¤: 0.5757, í›ˆë ¨ ì •í™•ë„: 76.62%\n",
            "ê²€ì¦ ì†ì‹¤: 0.5819, ê²€ì¦ ì •í™•ë„: 76.18%\n",
            "\n",
            "[ğŸ¨ Step2 ì—í­ 7/30]\n",
            "í›ˆë ¨ ì†ì‹¤: 0.5719, í›ˆë ¨ ì •í™•ë„: 76.62%\n",
            "ê²€ì¦ ì†ì‹¤: 0.5822, ê²€ì¦ ì •í™•ë„: 76.18%\n",
            "\n",
            "[ğŸ¨ Step2 ì—í­ 8/30]\n",
            "í›ˆë ¨ ì†ì‹¤: 0.5728, í›ˆë ¨ ì •í™•ë„: 76.62%\n",
            "ê²€ì¦ ì†ì‹¤: 0.5822, ê²€ì¦ ì •í™•ë„: 76.18%\n",
            "\n",
            "[ğŸ¨ Step2 ì—í­ 9/30]\n",
            "í›ˆë ¨ ì†ì‹¤: 0.5725, í›ˆë ¨ ì •í™•ë„: 76.62%\n",
            "ê²€ì¦ ì†ì‹¤: 0.5822, ê²€ì¦ ì •í™•ë„: 76.18%\n",
            "â° ì¡°ê¸° ì¢…ë£Œ\n",
            "\n",
            "ğŸ‰ ì „ì²´ í•™ìŠµ ì™„ë£Œ!\n",
            "ğŸ“¸ ì‚¬ì§„ ìµœê³  ê²€ì¦ ì •í™•ë„: 79.79%\n",
            "ğŸ¨ ê·¸ë¦¼ ìµœê³  ê²€ì¦ ì •í™•ë„: 76.18%\n"
          ]
        }
      ],
      "source": [
        "# Step 2: ê·¸ë¦¼ ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹\n",
        "print(f\"\\nğŸ¨ Step 2: ê·¸ë¦¼ ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹ ì‹œì‘ (ìµœê³  ì‚¬ì§„ ì •í™•ë„: {best_val_acc:.2f}%)\")\n",
        "model.load_state_dict(torch.load(\"best_photo_model.pth\"))\n",
        "\n",
        "# íŒŒì¸íŠœë‹ì„ ìœ„í•œ í•™ìŠµë¥  ì„¤ì •\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer_ft = torch.optim.AdamW(trainable_params, lr=LR_DRAWING, weight_decay=0.01)\n",
        "scheduler_ft = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_ft, T_max=EPOCHS_DRAWING)\n",
        "\n",
        "best_drawing_acc, patience_counter = 0, 0\n",
        "\n",
        "for epoch in range(EPOCHS_DRAWING):\n",
        "    print(f\"\\n[ğŸ¨ Step2 ì—í­ {epoch+1}/{EPOCHS_DRAWING}]\")\n",
        "\n",
        "    # 10 ì—í­ í›„ ë°±ë³¸ ë§ˆì§€ë§‰ ë ˆì´ì–´ë“¤ ì–¸í”„ë¦¬ì§•\n",
        "    if epoch == 10:\n",
        "        model.unfreeze_backbone_layers(num_layers=2)\n",
        "        \n",
        "        # ì˜µí‹°ë§ˆì´ì € ì¬ì„¤ì • (ì–¸í”„ë¦¬ì§•ëœ íŒŒë¼ë¯¸í„° í¬í•¨)\n",
        "        trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "        optimizer_ft = torch.optim.AdamW([\n",
        "            {'params': [p for n, p in model.named_parameters() if 'backbone' in n and p.requires_grad], 'lr': LR_DRAWING / 2},\n",
        "            {'params': [p for n, p in model.named_parameters() if 'head' in n], 'lr': LR_DRAWING}\n",
        "        ], weight_decay=0.01)\n",
        "        \n",
        "        # ìŠ¤ì¼€ì¤„ëŸ¬ë„ ì¬ì„¤ì •\n",
        "        scheduler_ft = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer_ft, T_max=EPOCHS_DRAWING - epoch\n",
        "        )\n",
        "\n",
        "    train_loss, train_acc = train_epoch(model, drawing_train_loader, optimizer_ft, criterion, scheduler_ft)\n",
        "    val_loss, val_acc, _, _ = evaluate_model(model, drawing_val_loader, criterion)\n",
        "\n",
        "    print(f\"í›ˆë ¨ ì†ì‹¤: {train_loss:.4f}, í›ˆë ¨ ì •í™•ë„: {train_acc:.2f}%\")\n",
        "    print(f\"ê²€ì¦ ì†ì‹¤: {val_loss:.4f}, ê²€ì¦ ì •í™•ë„: {val_acc:.2f}%\")\n",
        "\n",
        "    if val_acc > best_drawing_acc:\n",
        "        best_drawing_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_drawing_model.pth\")\n",
        "        patience_counter = 0\n",
        "        print(\"âœ… best_drawing_model.pth ì €ì¥ë¨\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= 8:\n",
        "            print(\"â° ì¡°ê¸° ì¢…ë£Œ\")\n",
        "            break\n",
        "\n",
        "print(f\"\\nğŸ‰ ì „ì²´ í•™ìŠµ ì™„ë£Œ!\")\n",
        "print(f\"ğŸ“¸ ì‚¬ì§„ ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_acc:.2f}%\")\n",
        "print(f\"ğŸ¨ ê·¸ë¦¼ ìµœê³  ê²€ì¦ ì •í™•ë„: {best_drawing_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“Š ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\n",
            "ìµœì¢… ê·¸ë¦¼ ê²€ì¦ ì •í™•ë„: 76.18%\n",
            "\n",
            "ğŸ“ˆ ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       train     0.7618    1.0000    0.8648       307\n",
            "         val     0.0000    0.0000    0.0000        96\n",
            "\n",
            "    accuracy                         0.7618       403\n",
            "   macro avg     0.3809    0.5000    0.4324       403\n",
            "weighted avg     0.5803    0.7618    0.6588       403\n",
            "\n",
            "\n",
            "âœ¨ í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/workspace/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "/workspace/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "/workspace/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        }
      ],
      "source": [
        "# ìµœì¢… ëª¨ë¸ ë¡œë“œ ë° í‰ê°€\n",
        "print(\"\\nğŸ“Š ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\")\n",
        "model.load_state_dict(torch.load(\"best_drawing_model.pth\"))\n",
        "\n",
        "# ê·¸ë¦¼ ë°ì´í„°ì…‹ì— ëŒ€í•œ ìƒì„¸ í‰ê°€\n",
        "val_loss, val_acc, predictions, true_labels = evaluate_model(model, drawing_val_loader, criterion)\n",
        "print(f\"ìµœì¢… ê·¸ë¦¼ ê²€ì¦ ì •í™•ë„: {val_acc:.2f}%\")\n",
        "\n",
        "# ë¶„ë¥˜ ë¦¬í¬íŠ¸ ì¶œë ¥\n",
        "class_names = drawing_dataset_train.classes\n",
        "print(\"\\nğŸ“ˆ ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
        "print(classification_report(true_labels, predictions, target_names=class_names, digits=4))\n",
        "\n",
        "print(\"\\nâœ¨ í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
