{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k1bbpB4pqC4w"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/workspace/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìå ÎîîÎ∞îÏù¥Ïä§: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "from sklearn.metrics import classification_report\n",
        "import timm\n",
        "import numpy as np\n",
        "\n",
        "# Í∏∞Ï°¥ Î™®Îç∏ ÌååÏùº ÏÇ≠Ï†ú\n",
        "for file in [\"photo_pretrained_dino.pth\", \"best_photo_model.pth\", \"best_drawing_model.pth\"]:\n",
        "    if os.path.exists(file):\n",
        "        os.remove(file)\n",
        "\n",
        "# ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üìå ÎîîÎ∞îÏù¥Ïä§: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aV3F48qhqK7q"
      },
      "outputs": [],
      "source": [
        "# ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞\n",
        "BATCH_SIZE = 16\n",
        "LR_PHOTO = 1e-4\n",
        "LR_DRAWING = 1e-5\n",
        "EPOCHS_PHOTO = 15\n",
        "EPOCHS_DRAWING = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "J5mG6s3RqME0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¨Í∏∞:\n",
            "ÏÇ¨ÏßÑ - ÌïôÏäµ: 5756, Í≤ÄÏ¶ù: 1440\n",
            "Í∑∏Î¶º - ÌïôÏäµ: 1608, Í≤ÄÏ¶ù: 403\n",
            "ÌÅ¥ÎûòÏä§: ['train', 'val']\n"
          ]
        }
      ],
      "source": [
        "# 224x224 Ï†ÑÏ≤òÎ¶¨ (DINOv2 ÏµúÏ†ÅÌôî)\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# train/val ÎÇòÎàÑÍ∏∞ (ÏùºÍ¥ÄÎêú Î∂ÑÌï†)\n",
        "def split_dataset_consistent(dataset_train, dataset_val):\n",
        "    total_size = len(dataset_train)\n",
        "    train_size = int(0.8 * total_size)\n",
        "    val_size = total_size - train_size\n",
        "\n",
        "    generator = torch.Generator().manual_seed(42)\n",
        "    train_indices, val_indices = random_split(range(total_size), [train_size, val_size], generator=generator)\n",
        "\n",
        "    train_subset = torch.utils.data.Subset(dataset_train, train_indices.indices)\n",
        "    val_subset = torch.utils.data.Subset(dataset_val, val_indices.indices)\n",
        "\n",
        "    return train_subset, val_subset\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú\n",
        "photo_dataset_train = datasets.ImageFolder('datasets/dataset2yolo/photo', transform=transform_train)\n",
        "photo_dataset_val = datasets.ImageFolder('datasets/dataset2yolo/photo', transform=transform_val)\n",
        "drawing_dataset_train = datasets.ImageFolder('datasets/dataset2yolo/drawing', transform=transform_train)\n",
        "drawing_dataset_val = datasets.ImageFolder('datasets/dataset2yolo/drawing', transform=transform_val)\n",
        "\n",
        "photo_train, photo_val = split_dataset_consistent(photo_dataset_train, photo_dataset_val)\n",
        "drawing_train, drawing_val = split_dataset_consistent(drawing_dataset_train, drawing_dataset_val)\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞ Î°úÎçî\n",
        "photo_train_loader = DataLoader(photo_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "photo_val_loader = DataLoader(photo_val, batch_size=BATCH_SIZE, num_workers=4)\n",
        "drawing_train_loader = DataLoader(drawing_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "drawing_val_loader = DataLoader(drawing_val, batch_size=BATCH_SIZE, num_workers=4)\n",
        "\n",
        "print(f\"üìä Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¨Í∏∞:\")\n",
        "print(f\"ÏÇ¨ÏßÑ - ÌïôÏäµ: {len(photo_train)}, Í≤ÄÏ¶ù: {len(photo_val)}\")\n",
        "print(f\"Í∑∏Î¶º - ÌïôÏäµ: {len(drawing_train)}, Í≤ÄÏ¶ù: {len(drawing_val)}\")\n",
        "print(f\"ÌÅ¥ÎûòÏä§: {photo_dataset_train.classes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC97lwhgqYDy"
      },
      "source": [
        "### Dinov2 + Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9SbvwDwhqoK"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cKMlypXSqRb8"
      },
      "outputs": [],
      "source": [
        "class DinoTransformerClassifier(nn.Module):\n",
        "    def __init__(self, model_name='vit_small_patch14_dinov2.lvd142m', num_classes=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        # DINOv2 Î∞±Î≥∏ Î°úÎìú (ÏÇ¨Ï†ÑÌõàÎ†®Îêú ÌäπÏÑ± Ï∂îÏ∂úÍ∏∞)\n",
        "        self.backbone = timm.create_model(model_name, pretrained=True, img_size=224)\n",
        "        self.backbone.reset_classifier(0)  # Í∏∞Ï°¥ Î∂ÑÎ•òÍ∏∞ Ï†úÍ±∞\n",
        "        \n",
        "        # Ï¥àÍ∏∞ÏóêÎäî Î∞±Î≥∏ Í∞ÄÏ§ëÏπò Í≥†Ï†ï (ÌäπÏÑ± Ï∂îÏ∂úÍ∏∞Î°úÎßå ÏÇ¨Ïö©)\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Î∂ÑÎ•ò Ìó§Îìú (Í∞úÏÑ†Îêú Íµ¨Ï°∞)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(self.backbone.num_features),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(self.backbone.num_features, 512),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout * 1.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # DINOv2 ÌäπÏÑ± Ï∂îÏ∂ú\n",
        "        features = self.backbone.forward_features(x)  # [B, num_patches+1, embed_dim]\n",
        "        \n",
        "        # CLS ÌÜ†ÌÅ∞ Ï∂îÏ∂ú (Ï≤´ Î≤àÏß∏ ÌÜ†ÌÅ∞)\n",
        "        cls_token = features[:, 0]  # [B, embed_dim]\n",
        "        \n",
        "        # Î∂ÑÎ•ò Ìó§Îìú ÌÜµÍ≥º\n",
        "        return self.head(cls_token)\n",
        "\n",
        "    def unfreeze_backbone_layers(self, num_layers=2):\n",
        "        \"\"\"Î∞±Î≥∏Ïùò ÎßàÏßÄÎßâ Î†àÏù¥Ïñ¥Îì§ÏùÑ Ïñ∏ÌîÑÎ¶¨Ïßï\"\"\"\n",
        "        total_blocks = len(self.backbone.blocks)\n",
        "        for i in range(total_blocks - num_layers, total_blocks):\n",
        "            for param in self.backbone.blocks[i].parameters():\n",
        "                param.requires_grad = True\n",
        "        \n",
        "        # norm layerÎèÑ Ïñ∏ÌîÑÎ¶¨Ïßï\n",
        "        for param in self.backbone.norm.parameters():\n",
        "            param.requires_grad = True\n",
        "        \n",
        "        print(f\"üîì Î∞±Î≥∏Ïùò ÎßàÏßÄÎßâ {num_layers}Í∞ú Î†àÏù¥Ïñ¥ Ïñ∏ÌîÑÎ¶¨Ïßï ÏôÑÎ£å\")\n",
        "\n",
        "# ÏÜêÏã§Ìï®Ïàò\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "def train_epoch(model, loader, optimizer, criterion, scheduler=None):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        \n",
        "        # Gradient clipping (ÏïàÏ†ïÏ†ÅÏù∏ ÌïôÏäµ)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    if scheduler:\n",
        "        scheduler.step()\n",
        "\n",
        "    return running_loss / len(loader), 100. * correct / total\n",
        "\n",
        "def evaluate_model(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            \n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return running_loss / len(loader), 100. * correct / total, all_predictions, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aiNBLEUfqr7Q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üöÄ Step 1: ÏÇ¨ÏßÑ Îç∞Ïù¥ÌÑ∞Î°ú Ï¥àÍ∏∞ ÌïôÏäµ ÏãúÏûë\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[üì∏ Step1 ÏóêÌè≠ 1/15]\n",
            "ÌõàÎ†® ÏÜêÏã§: 0.5473, ÌõàÎ†® Ï†ïÌôïÎèÑ: 79.93%\n",
            "Í≤ÄÏ¶ù ÏÜêÏã§: 0.5447, Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: 79.79%\n",
            "‚úÖ best_photo_model.pth Ï†ÄÏû•Îê®\n",
            "\n",
            "[üì∏ Step1 ÏóêÌè≠ 2/15]\n",
            "ÌõàÎ†® ÏÜêÏã§: 0.5389, ÌõàÎ†® Ï†ïÌôïÎèÑ: 80.04%\n",
            "Í≤ÄÏ¶ù ÏÜêÏã§: 0.5425, Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: 79.79%\n",
            "\n",
            "[üì∏ Step1 ÏóêÌè≠ 3/15]\n",
            "ÌõàÎ†® ÏÜêÏã§: 0.5355, ÌõàÎ†® Ï†ïÌôïÎèÑ: 80.04%\n",
            "Í≤ÄÏ¶ù ÏÜêÏã§: 0.5432, Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: 79.79%\n",
            "\n",
            "[üì∏ Step1 ÏóêÌè≠ 4/15]\n",
            "ÌõàÎ†® ÏÜêÏã§: 0.5332, ÌõàÎ†® Ï†ïÌôïÎèÑ: 80.04%\n",
            "Í≤ÄÏ¶ù ÏÜêÏã§: 0.5477, Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: 79.79%\n",
            "\n",
            "[üì∏ Step1 ÏóêÌè≠ 5/15]\n",
            "ÌõàÎ†® ÏÜêÏã§: 0.5305, ÌõàÎ†® Ï†ïÌôïÎèÑ: 80.04%\n",
            "Í≤ÄÏ¶ù ÏÜêÏã§: 0.5519, Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: 79.79%\n",
            "\n",
            "[üì∏ Step1 ÏóêÌè≠ 6/15]\n",
            "ÌõàÎ†® ÏÜêÏã§: 0.5271, ÌõàÎ†® Ï†ïÌôïÎèÑ: 80.06%\n",
            "Í≤ÄÏ¶ù ÏÜêÏã§: 0.5496, Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: 79.72%\n",
            "\n",
            "[üì∏ Step1 ÏóêÌè≠ 7/15]\n",
            "ÌõàÎ†® ÏÜêÏã§: 0.5230, ÌõàÎ†® Ï†ïÌôïÎèÑ: 80.04%\n",
            "Í≤ÄÏ¶ù ÏÜêÏã§: 0.5497, Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: 79.72%\n",
            "\n",
            "[üì∏ Step1 ÏóêÌè≠ 8/15]\n",
            "ÌõàÎ†® ÏÜêÏã§: 0.5183, ÌõàÎ†® Ï†ïÌôïÎèÑ: 80.09%\n",
            "Í≤ÄÏ¶ù ÏÜêÏã§: 0.5541, Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: 79.72%\n",
            "‚è∞ Ï°∞Í∏∞ Ï¢ÖÎ£å\n"
          ]
        }
      ],
      "source": [
        "# Step 1: ÏÇ¨ÏßÑ Îç∞Ïù¥ÌÑ∞Î°ú Î™®Îç∏ ÌïôÏäµ\n",
        "print(\"\\nüöÄ Step 1: ÏÇ¨ÏßÑ Îç∞Ïù¥ÌÑ∞Î°ú Ï¥àÍ∏∞ ÌïôÏäµ ÏãúÏûë\")\n",
        "model = DinoTransformerClassifier(num_classes=len(photo_dataset_train.classes)).to(device)\n",
        "\n",
        "# ÌïôÏäµ Í∞ÄÎä•Ìïú ÌååÎùºÎØ∏ÌÑ∞Îßå ÏµúÏ†ÅÌôî (Ï≤òÏùåÏóêÎäî headÎßå)\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.AdamW(trainable_params, lr=LR_PHOTO, weight_decay=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_PHOTO)\n",
        "\n",
        "best_val_acc, patience_counter = 0, 0\n",
        "\n",
        "for epoch in range(EPOCHS_PHOTO):\n",
        "    print(f\"\\n[üì∏ Step1 ÏóêÌè≠ {epoch+1}/{EPOCHS_PHOTO}]\")\n",
        "    train_loss, train_acc = train_epoch(model, photo_train_loader, optimizer, criterion, scheduler)\n",
        "    val_loss, val_acc, _, _ = evaluate_model(model, photo_val_loader, criterion)\n",
        "\n",
        "    print(f\"ÌõàÎ†® ÏÜêÏã§: {train_loss:.4f}, ÌõàÎ†® Ï†ïÌôïÎèÑ: {train_acc:.2f}%\")\n",
        "    print(f\"Í≤ÄÏ¶ù ÏÜêÏã§: {val_loss:.4f}, Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: {val_acc:.2f}%\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_photo_model.pth\")\n",
        "        patience_counter = 0\n",
        "        print(\"‚úÖ best_photo_model.pth Ï†ÄÏû•Îê®\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= 7:\n",
        "            print(\"‚è∞ Ï°∞Í∏∞ Ï¢ÖÎ£å\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PV6_VvliquoZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üé® Step 2: Í∑∏Î¶º Îç∞Ïù¥ÌÑ∞Î°ú ÌååÏù∏ÌäúÎãù ÏãúÏûë (ÏµúÍ≥† ÏÇ¨ÏßÑ Ï†ïÌôïÎèÑ: 79.79%)\n",
            "\n",
            "[üé® Step2 ÏóêÌè≠ 1/30]\n",
            "ÌõàÎ†® ÏÜêÏã§: 0.5845, ÌõàÎ†® Ï†ïÌôïÎèÑ: 76.62%\n",
            "Í≤ÄÏ¶ù ÏÜêÏã§: 0.5870, Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: 76.18%\n",
            "‚úÖ best_drawing_model.pth Ï†ÄÏû•Îê®\n",
            "\n",
            "[üé® Step2 ÏóêÌè≠ 2/30]\n",
            "ÌõàÎ†® ÏÜêÏã§: 0.5766, ÌõàÎ†® Ï†ïÌôïÎèÑ: 76.62%\n",
            "Í≤ÄÏ¶ù ÏÜêÏã§: 0.5831, Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: 76.18%\n",
            "\n",
            "[üé® Step2 ÏóêÌè≠ 3/30]\n",
            "ÌõàÎ†® ÏÜêÏã§: 0.5759, ÌõàÎ†® Ï†ïÌôïÎèÑ: 76.62%\n",
            "Í≤ÄÏ¶ù ÏÜêÏã§: 0.5822, Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: 76.18%\n",
            "\n",
            "[üé® Step2 ÏóêÌè≠ 4/30]\n",
            "ÌõàÎ†® ÏÜêÏã§: 0.5753, ÌõàÎ†® Ï†ïÌôïÎèÑ: 76.62%\n",
            "Í≤ÄÏ¶ù ÏÜêÏã§: 0.5818, Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: 76.18%\n",
            "\n",
            "[üé® Step2 ÏóêÌè≠ 5/30]\n",
            "ÌõàÎ†® ÏÜêÏã§: 0.5744, ÌõàÎ†® Ï†ïÌôïÎèÑ: 76.62%\n",
            "Í≤ÄÏ¶ù ÏÜêÏã§: 0.5819, Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: 76.18%\n",
            "\n",
            "[üé® Step2 ÏóêÌè≠ 6/30]\n",
            "ÌõàÎ†® ÏÜêÏã§: 0.5757, ÌõàÎ†® Ï†ïÌôïÎèÑ: 76.62%\n",
            "Í≤ÄÏ¶ù ÏÜêÏã§: 0.5819, Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: 76.18%\n",
            "\n",
            "[üé® Step2 ÏóêÌè≠ 7/30]\n",
            "ÌõàÎ†® ÏÜêÏã§: 0.5719, ÌõàÎ†® Ï†ïÌôïÎèÑ: 76.62%\n",
            "Í≤ÄÏ¶ù ÏÜêÏã§: 0.5822, Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: 76.18%\n",
            "\n",
            "[üé® Step2 ÏóêÌè≠ 8/30]\n",
            "ÌõàÎ†® ÏÜêÏã§: 0.5728, ÌõàÎ†® Ï†ïÌôïÎèÑ: 76.62%\n",
            "Í≤ÄÏ¶ù ÏÜêÏã§: 0.5822, Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: 76.18%\n",
            "\n",
            "[üé® Step2 ÏóêÌè≠ 9/30]\n",
            "ÌõàÎ†® ÏÜêÏã§: 0.5725, ÌõàÎ†® Ï†ïÌôïÎèÑ: 76.62%\n",
            "Í≤ÄÏ¶ù ÏÜêÏã§: 0.5822, Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: 76.18%\n",
            "‚è∞ Ï°∞Í∏∞ Ï¢ÖÎ£å\n",
            "\n",
            "üéâ Ï†ÑÏ≤¥ ÌïôÏäµ ÏôÑÎ£å!\n",
            "üì∏ ÏÇ¨ÏßÑ ÏµúÍ≥† Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: 79.79%\n",
            "üé® Í∑∏Î¶º ÏµúÍ≥† Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: 76.18%\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Í∑∏Î¶º Îç∞Ïù¥ÌÑ∞Î°ú ÌååÏù∏ÌäúÎãù\n",
        "print(f\"\\nüé® Step 2: Í∑∏Î¶º Îç∞Ïù¥ÌÑ∞Î°ú ÌååÏù∏ÌäúÎãù ÏãúÏûë (ÏµúÍ≥† ÏÇ¨ÏßÑ Ï†ïÌôïÎèÑ: {best_val_acc:.2f}%)\")\n",
        "model.load_state_dict(torch.load(\"best_photo_model.pth\"))\n",
        "\n",
        "# ÌååÏù∏ÌäúÎãùÏùÑ ÏúÑÌïú ÌïôÏäµÎ•† ÏÑ§Ï†ï\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer_ft = torch.optim.AdamW(trainable_params, lr=LR_DRAWING, weight_decay=0.01)\n",
        "scheduler_ft = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_ft, T_max=EPOCHS_DRAWING)\n",
        "\n",
        "best_drawing_acc, patience_counter = 0, 0\n",
        "\n",
        "for epoch in range(EPOCHS_DRAWING):\n",
        "    print(f\"\\n[üé® Step2 ÏóêÌè≠ {epoch+1}/{EPOCHS_DRAWING}]\")\n",
        "\n",
        "    # 10 ÏóêÌè≠ ÌõÑ Î∞±Î≥∏ ÎßàÏßÄÎßâ Î†àÏù¥Ïñ¥Îì§ Ïñ∏ÌîÑÎ¶¨Ïßï\n",
        "    if epoch == 10:\n",
        "        model.unfreeze_backbone_layers(num_layers=2)\n",
        "        \n",
        "        # ÏòµÌã∞ÎßàÏù¥Ï†Ä Ïû¨ÏÑ§Ï†ï (Ïñ∏ÌîÑÎ¶¨ÏßïÎêú ÌååÎùºÎØ∏ÌÑ∞ Ìè¨Ìï®)\n",
        "        trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "        optimizer_ft = torch.optim.AdamW([\n",
        "            {'params': [p for n, p in model.named_parameters() if 'backbone' in n and p.requires_grad], 'lr': LR_DRAWING / 2},\n",
        "            {'params': [p for n, p in model.named_parameters() if 'head' in n], 'lr': LR_DRAWING}\n",
        "        ], weight_decay=0.01)\n",
        "        \n",
        "        # Ïä§ÏºÄÏ§ÑÎü¨ÎèÑ Ïû¨ÏÑ§Ï†ï\n",
        "        scheduler_ft = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer_ft, T_max=EPOCHS_DRAWING - epoch\n",
        "        )\n",
        "\n",
        "    train_loss, train_acc = train_epoch(model, drawing_train_loader, optimizer_ft, criterion, scheduler_ft)\n",
        "    val_loss, val_acc, _, _ = evaluate_model(model, drawing_val_loader, criterion)\n",
        "\n",
        "    print(f\"ÌõàÎ†® ÏÜêÏã§: {train_loss:.4f}, ÌõàÎ†® Ï†ïÌôïÎèÑ: {train_acc:.2f}%\")\n",
        "    print(f\"Í≤ÄÏ¶ù ÏÜêÏã§: {val_loss:.4f}, Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: {val_acc:.2f}%\")\n",
        "\n",
        "    if val_acc > best_drawing_acc:\n",
        "        best_drawing_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_drawing_model.pth\")\n",
        "        patience_counter = 0\n",
        "        print(\"‚úÖ best_drawing_model.pth Ï†ÄÏû•Îê®\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= 8:\n",
        "            print(\"‚è∞ Ï°∞Í∏∞ Ï¢ÖÎ£å\")\n",
        "            break\n",
        "\n",
        "print(f\"\\nüéâ Ï†ÑÏ≤¥ ÌïôÏäµ ÏôÑÎ£å!\")\n",
        "print(f\"üì∏ ÏÇ¨ÏßÑ ÏµúÍ≥† Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: {best_val_acc:.2f}%\")\n",
        "print(f\"üé® Í∑∏Î¶º ÏµúÍ≥† Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: {best_drawing_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä ÏµúÏ¢Ö Î™®Îç∏ ÏÑ±Îä• ÌèâÍ∞Ä\n",
            "ÏµúÏ¢Ö Í∑∏Î¶º Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: 76.18%\n",
            "\n",
            "üìà ÏÉÅÏÑ∏ Î∂ÑÎ•ò Î¶¨Ìè¨Ìä∏:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       train     0.7618    1.0000    0.8648       307\n",
            "         val     0.0000    0.0000    0.0000        96\n",
            "\n",
            "    accuracy                         0.7618       403\n",
            "   macro avg     0.3809    0.5000    0.4324       403\n",
            "weighted avg     0.5803    0.7618    0.6588       403\n",
            "\n",
            "\n",
            "‚ú® ÌïôÏäµ ÏôÑÎ£å! Î™®Îç∏Ïù¥ Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/workspace/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "/workspace/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "/workspace/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        }
      ],
      "source": [
        "# ÏµúÏ¢Ö Î™®Îç∏ Î°úÎìú Î∞è ÌèâÍ∞Ä\n",
        "print(\"\\nüìä ÏµúÏ¢Ö Î™®Îç∏ ÏÑ±Îä• ÌèâÍ∞Ä\")\n",
        "model.load_state_dict(torch.load(\"best_drawing_model.pth\"))\n",
        "\n",
        "# Í∑∏Î¶º Îç∞Ïù¥ÌÑ∞ÏÖãÏóê ÎåÄÌïú ÏÉÅÏÑ∏ ÌèâÍ∞Ä\n",
        "val_loss, val_acc, predictions, true_labels = evaluate_model(model, drawing_val_loader, criterion)\n",
        "print(f\"ÏµúÏ¢Ö Í∑∏Î¶º Í≤ÄÏ¶ù Ï†ïÌôïÎèÑ: {val_acc:.2f}%\")\n",
        "\n",
        "# Î∂ÑÎ•ò Î¶¨Ìè¨Ìä∏ Ï∂úÎ†•\n",
        "class_names = drawing_dataset_train.classes\n",
        "print(\"\\nüìà ÏÉÅÏÑ∏ Î∂ÑÎ•ò Î¶¨Ìè¨Ìä∏:\")\n",
        "print(classification_report(true_labels, predictions, target_names=class_names, digits=4))\n",
        "\n",
        "print(\"\\n‚ú® ÌïôÏäµ ÏôÑÎ£å! Î™®Îç∏Ïù¥ Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
